{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>110992</td>\n",
       "      <td>If Music Be the Food of Love... Then Prepare f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76494</td>\n",
       "      <td>Gil Elvgren Tribute \\n\\nI was just watching th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47429</td>\n",
       "      <td>Thank you very much! )</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49486</td>\n",
       "      <td>Your Advisor\\nA little bit more context for th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98028</td>\n",
       "      <td>Notability \\n\\nHi there; you have unerringly s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "110992  If Music Be the Food of Love... Then Prepare f...      0\n",
       "76494   Gil Elvgren Tribute \\n\\nI was just watching th...      0\n",
       "47429                              Thank you very much! )      0\n",
       "49486   Your Advisor\\nA little bit more context for th...      0\n",
       "98028   Notability \\n\\nHi there; you have unerringly s...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data.info()\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"explanation\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].str.lower()\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк на класс:  [143316  16210]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.898386\n",
       "1    0.101614\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data.duplicated().sum() != 0:\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "print('Количество строк на класс: ', np.bincount(data['toxic']))\n",
    "data['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборка несбалансирована. Возможно стоит применить downsampling на такой большой выборке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explan whi the edit made under my usernam hardcor metallica fan were revert they were n't vandal just closur on some gas after i vote at new york doll fac and pleas do n't remov the templat from the talk page sinc i 'm retir now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"explanation why the edits make under my username hardcore metallica fan be revert they be n't vandalisms just closure on some gas after i vote at new york doll fac and please do n't remove the template from the talk page since i 'm retire now\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def clean_token(text):\n",
    "    \n",
    "    text = re.sub(r'[^a-z\\' ]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def make_lemm(text):\n",
    "    \n",
    "    lemmer = WordNetLemmatizer()\n",
    "    tag_tokens = nltk.pos_tag(clean_token(text))\n",
    "    \n",
    "    sentence = [lemmer.lemmatize(token, \n",
    "                                 pos=((tag[0].lower() if tag[0].lower() in 'nvr' else 'n') if tag[0].lower()!='j' else 's')) \n",
    "                for token, tag in tag_tokens]\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "def make_stem(text):\n",
    "    \n",
    "    stemmer = EnglishStemmer()\n",
    "    sentence = [stemmer.stem(w) for w in clean_token(text)]\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "print(make_stem(data['text'][0]))\n",
    "make_lemm(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 35s, sys: 792 ms, total: 3min 36s\n",
      "Wall time: 3min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "target = data['toxic']\n",
    "feat = data['text'].apply(make_stem)\n",
    "len(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 6s, sys: 6.73 s, total: 12min 13s\n",
      "Wall time: 12min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159526"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "feat_lemm = data['text'].apply(make_lemm)\n",
    "len(feat_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация долгая из-за кучи условий, а так она даже быстрее чем стемминг в 2 раза была.\n",
    "\n",
    "---\n",
    "В tf-idf можно же отбросить редко встречающиеся слова? Хотя, судя по несбалансированности, это как раз могут оказаться слова-признаки указывающие на токсичность. \n",
    "\n",
    "Стоит посмотреть на сами токсичные комментарии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"\\nlook, the anglophobic vigilante has come bearing ill news and \"\"ill news is an ill guest\"\".  watch out, tyke, he considers himself to have inviolate powers of edit warring upon your contributions, now that he sees some other people have an unrelated dispute with you.  all the more \"\"justification\"\" for him to run roughshod over you to get what he wants.  it\\'s been done before.  just check at my own edits for proof.    \"',\n",
       "       \"dickhead\\nyou know it was stupid, you know it wasn't clever. so why do it? 94.195.251.61\",\n",
       "       \"u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'u suck!!!!\\nu suck!!!!'u suck!!!!u suck!!!!'\",\n",
       "       'i think it means: me turk man, me vill rape bad woomen, me vill teach dem a lesson, me vill cover der faces in gauze. me vill beat dose harlots up! s',\n",
       "       'nightstallions wife got fucked by a nigger and had his baby\\n\\nand it smelled of fried chicken'],\n",
       "      dtype='<U2752')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['toxic'] == 1]['text'].sample(5).values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf вместо 5000 слов! Tf для слова fucksex будет 0.99, а idf - примерно 4.2 (скорее всего это редкое слово и более чем в 10 документах его не будет). Tf-idf ~ 4.16, вот такие слова, с высоким значением tf-idf тут и нужны (это еще раз к тому что downsampling плохая идея). Несбалансированность тут как раз и создает признак.\n",
    "\n",
    "Исходя из текстов могут быть полезны N-граммы от 1 до 4 \n",
    "- Что если просто ограничить число фичей?\n",
    "- А если попробовать отобрать признаки из этой кучи по какоиму нибудь правилу? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стеммы: (106882, 100000) (106882,) (52644, 100000) (52644,)\n",
      "Леммы: (106882, 100000) (106882,) (52644, 100000) (52644,)\n",
      "CPU times: user 2min 48s, sys: 6.67 s, total: 2min 55s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def tf_idf_vect(feat, target):\n",
    "    \n",
    "    stp_wrds = set(stopwords.words('english'))\n",
    "    \n",
    "    tf_idf = TfidfVectorizer(min_df=2, max_df=0.9, max_features=500000, \n",
    "                             ngram_range=(1, 4), sublinear_tf=True,\n",
    "                             stop_words=stp_wrds)\n",
    "    select = SelectKBest(chi2, k=100000)\n",
    "    \n",
    "    feat_train, feat_test, y_train, y_test = train_test_split(feat, target, test_size=0.33, random_state=42)\n",
    "    \n",
    "    tf_train = tf_idf.fit_transform(feat_train)\n",
    "    tf_test = tf_idf.transform(feat_test)\n",
    "    \n",
    "    tf_train_new = select.fit_transform(tf_train, y_train)\n",
    "    tf_test_new = select.transform(tf_test)\n",
    "    \n",
    "    return tf_train_new, tf_test_new, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "tf_train, tf_test, y_train, y_test = tf_idf_vect(feat, target)\n",
    "train_lemm, test_lemm, y_train, y_test = tf_idf_vect(feat_lemm, target)\n",
    "\n",
    "print('Стеммы:', tf_train.shape, y_train.shape, tf_test.shape, y_test.shape)\n",
    "print('Леммы:', train_lemm.shape, y_train.shape, test_lemm.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe = Pipeline([('clf', LogisticRegression())])\n",
    "\n",
    "pipe_grid = [\n",
    "    {'clf': [LogisticRegression(solver='sag', max_iter=5000, \n",
    "                                class_weight='balanced', random_state=42)],\n",
    "     'clf__C':             [1, 5, 10, 12]\n",
    "    }, \n",
    "    {'clf': [SGDClassifier(class_weight='balanced', max_iter=5000, learning_rate='adaptive', \n",
    "                           early_stopping=True, eta0=0.5, random_state=42)],\n",
    "     'clf__loss':          ['modified_huber', 'squared_hinge'],\n",
    "     'clf__eta0':          [0.5, 0.1, 1, 2, 3, 4]\n",
    "    },\n",
    "    {'clf': [MultinomialNB()],\n",
    "     'clf__alpha':         [0.6, 0.3, 0.1, 1],\n",
    "     'clf__class_prior':   [[1, 2], None]\n",
    "    }\n",
    "         ]\n",
    "\n",
    "def search_model(pipe, pipe_grid, X, y, test, y_test):\n",
    "    \n",
    "    grid_search = GridSearchCV(pipe, pipe_grid, cv=5, scoring='f1')\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    best_estim = grid_search.best_estimator_.named_steps['clf']\n",
    "    \n",
    "    print('Лучшая модель:\\n', best_estim)\n",
    "    print('F1 Score: ', grid_search.best_score_)\n",
    "    print('Test F1 Score: ', grid_search.score(test, y_test))\n",
    "    \n",
    "    cv_reslt = pd.DataFrame(grid_search.cv_results_\n",
    "                           ).sort_values(by='rank_test_score')\n",
    "    \n",
    "    return cv_reslt[\n",
    "        [col for col in cv_reslt.columns \n",
    "         if 'param_' in col] + \n",
    "        ['mean_fit_time', 'mean_test_score']\n",
    "                   ], best_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На леммах:\n",
      "Лучшая модель:\n",
      " LogisticRegression(C=12, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=5000, multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='sag', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "F1 Score:  0.7891661234045364\n",
      "Test F1 Score:  0.7747631748367516\n",
      "CPU times: user 10min 34s, sys: 6.62 s, total: 10min 40s\n",
      "Wall time: 10min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__eta0</th>\n",
       "      <th>param_clf__loss</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__class_prior</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.261751</td>\n",
       "      <td>0.789166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.155937</td>\n",
       "      <td>0.789107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.268415</td>\n",
       "      <td>0.787378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.325961</td>\n",
       "      <td>0.770786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.361076</td>\n",
       "      <td>0.770726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.841924</td>\n",
       "      <td>0.770713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.856032</td>\n",
       "      <td>0.770651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.556110</td>\n",
       "      <td>0.770649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.857775</td>\n",
       "      <td>0.770606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.864246</td>\n",
       "      <td>0.770558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.788898</td>\n",
       "      <td>0.770539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.722192</td>\n",
       "      <td>0.770531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.627111</td>\n",
       "      <td>0.770507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.583703</td>\n",
       "      <td>0.770443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.587660</td>\n",
       "      <td>0.770398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.528528</td>\n",
       "      <td>0.761453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.062460</td>\n",
       "      <td>0.750312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.716863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.061326</td>\n",
       "      <td>0.698299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.061263</td>\n",
       "      <td>0.696929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.060630</td>\n",
       "      <td>0.654856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.063414</td>\n",
       "      <td>0.613646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>0.594561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.060860</td>\n",
       "      <td>0.491137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            param_clf param_clf__C  \\\n",
       "3   LogisticRegression(C=12, class_weight='balance...           12   \n",
       "2   LogisticRegression(C=12, class_weight='balance...           10   \n",
       "1   LogisticRegression(C=12, class_weight='balance...            5   \n",
       "6   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "7   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "11  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "10  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "5   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "13  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "12  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "14  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "15  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "8   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "9   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "4   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "0   LogisticRegression(C=12, class_weight='balance...            1   \n",
       "21  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "22  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "16  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "19  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "18  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "20  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "17  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "23  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "\n",
       "   param_clf__eta0 param_clf__loss param_clf__alpha param_clf__class_prior  \\\n",
       "3              NaN             NaN              NaN                    NaN   \n",
       "2              NaN             NaN              NaN                    NaN   \n",
       "1              NaN             NaN              NaN                    NaN   \n",
       "6              0.1  modified_huber              NaN                    NaN   \n",
       "7              0.1   squared_hinge              NaN                    NaN   \n",
       "11               2   squared_hinge              NaN                    NaN   \n",
       "10               2  modified_huber              NaN                    NaN   \n",
       "5              0.5   squared_hinge              NaN                    NaN   \n",
       "13               3   squared_hinge              NaN                    NaN   \n",
       "12               3  modified_huber              NaN                    NaN   \n",
       "14               4  modified_huber              NaN                    NaN   \n",
       "15               4   squared_hinge              NaN                    NaN   \n",
       "8                1  modified_huber              NaN                    NaN   \n",
       "9                1   squared_hinge              NaN                    NaN   \n",
       "4              0.5  modified_huber              NaN                    NaN   \n",
       "0              NaN             NaN              NaN                    NaN   \n",
       "21             NaN             NaN              0.1                   None   \n",
       "22             NaN             NaN                1                 [1, 2]   \n",
       "16             NaN             NaN              0.6                 [1, 2]   \n",
       "19             NaN             NaN              0.3                   None   \n",
       "18             NaN             NaN              0.3                 [1, 2]   \n",
       "20             NaN             NaN              0.1                 [1, 2]   \n",
       "17             NaN             NaN              0.6                   None   \n",
       "23             NaN             NaN                1                   None   \n",
       "\n",
       "    mean_fit_time  mean_test_score  \n",
       "3       29.261751         0.789166  \n",
       "2       18.155937         0.789107  \n",
       "1       20.268415         0.787378  \n",
       "6        2.325961         0.770786  \n",
       "7        2.361076         0.770726  \n",
       "11       2.841924         0.770713  \n",
       "10       2.856032         0.770651  \n",
       "5        2.556110         0.770649  \n",
       "13       2.857775         0.770606  \n",
       "12       2.864246         0.770558  \n",
       "14       2.788898         0.770539  \n",
       "15       2.722192         0.770531  \n",
       "8        2.627111         0.770507  \n",
       "9        2.583703         0.770443  \n",
       "4        2.587660         0.770398  \n",
       "0        3.528528         0.761453  \n",
       "21       0.062460         0.750312  \n",
       "22       0.061181         0.716863  \n",
       "16       0.061326         0.698299  \n",
       "19       0.061263         0.696929  \n",
       "18       0.060630         0.654856  \n",
       "20       0.063414         0.613646  \n",
       "17       0.061271         0.594561  \n",
       "23       0.060860         0.491137  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print('На леммах:')\n",
    "cv_result, best_estim = search_model(pipe, pipe_grid, train_lemm, y_train, test_lemm, y_test)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На стеммах:\n",
      "Лучшая модель:\n",
      " LogisticRegression(C=12, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=5000, multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='sag', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "F1 Score:  0.79695414199236\n",
      "Test F1 Score:  0.7869817045141123\n",
      "CPU times: user 10min 2s, sys: 6.3 s, total: 10min 8s\n",
      "Wall time: 10min 9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__eta0</th>\n",
       "      <th>param_clf__loss</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__class_prior</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.556702</td>\n",
       "      <td>0.796954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.541810</td>\n",
       "      <td>0.795836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.706217</td>\n",
       "      <td>0.794496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.849763</td>\n",
       "      <td>0.780905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.631774</td>\n",
       "      <td>0.780720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.875435</td>\n",
       "      <td>0.780680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.822159</td>\n",
       "      <td>0.780575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.631151</td>\n",
       "      <td>0.780528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.820902</td>\n",
       "      <td>0.780434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.766557</td>\n",
       "      <td>0.780430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.805678</td>\n",
       "      <td>0.780420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.664600</td>\n",
       "      <td>0.780388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.366219</td>\n",
       "      <td>0.780345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.403737</td>\n",
       "      <td>0.780260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SGDClassifier(alpha=0.0001, average=False, cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>modified_huber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.616092</td>\n",
       "      <td>0.780236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression(C=12, class_weight='balance...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.653663</td>\n",
       "      <td>0.771314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>0.756486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.063360</td>\n",
       "      <td>0.725323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.063463</td>\n",
       "      <td>0.704219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>0.702678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.658448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>0.064460</td>\n",
       "      <td>0.613555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>None</td>\n",
       "      <td>0.063135</td>\n",
       "      <td>0.598527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>MultinomialNB(alpha=1, class_prior=None, fit_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.063774</td>\n",
       "      <td>0.493821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            param_clf param_clf__C  \\\n",
       "3   LogisticRegression(C=12, class_weight='balance...           12   \n",
       "2   LogisticRegression(C=12, class_weight='balance...           10   \n",
       "1   LogisticRegression(C=12, class_weight='balance...            5   \n",
       "11  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "9   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "10  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "12  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "8   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "14  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "15  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "13  SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "5   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "7   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "6   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "4   SGDClassifier(alpha=0.0001, average=False, cla...          NaN   \n",
       "0   LogisticRegression(C=12, class_weight='balance...            1   \n",
       "21  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "22  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "16  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "19  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "18  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "20  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "17  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "23  MultinomialNB(alpha=1, class_prior=None, fit_p...          NaN   \n",
       "\n",
       "   param_clf__eta0 param_clf__loss param_clf__alpha param_clf__class_prior  \\\n",
       "3              NaN             NaN              NaN                    NaN   \n",
       "2              NaN             NaN              NaN                    NaN   \n",
       "1              NaN             NaN              NaN                    NaN   \n",
       "11               2   squared_hinge              NaN                    NaN   \n",
       "9                1   squared_hinge              NaN                    NaN   \n",
       "10               2  modified_huber              NaN                    NaN   \n",
       "12               3  modified_huber              NaN                    NaN   \n",
       "8                1  modified_huber              NaN                    NaN   \n",
       "14               4  modified_huber              NaN                    NaN   \n",
       "15               4   squared_hinge              NaN                    NaN   \n",
       "13               3   squared_hinge              NaN                    NaN   \n",
       "5              0.5   squared_hinge              NaN                    NaN   \n",
       "7              0.1   squared_hinge              NaN                    NaN   \n",
       "6              0.1  modified_huber              NaN                    NaN   \n",
       "4              0.5  modified_huber              NaN                    NaN   \n",
       "0              NaN             NaN              NaN                    NaN   \n",
       "21             NaN             NaN              0.1                   None   \n",
       "22             NaN             NaN                1                 [1, 2]   \n",
       "16             NaN             NaN              0.6                 [1, 2]   \n",
       "19             NaN             NaN              0.3                   None   \n",
       "18             NaN             NaN              0.3                 [1, 2]   \n",
       "20             NaN             NaN              0.1                 [1, 2]   \n",
       "17             NaN             NaN              0.6                   None   \n",
       "23             NaN             NaN                1                   None   \n",
       "\n",
       "    mean_fit_time  mean_test_score  \n",
       "3       17.556702         0.796954  \n",
       "2       27.541810         0.795836  \n",
       "1       21.706217         0.794496  \n",
       "11       2.849763         0.780905  \n",
       "9        2.631774         0.780720  \n",
       "10       2.875435         0.780680  \n",
       "12       2.822159         0.780575  \n",
       "8        2.631151         0.780528  \n",
       "14       2.820902         0.780434  \n",
       "15       2.766557         0.780430  \n",
       "13       2.805678         0.780420  \n",
       "5        2.664600         0.780388  \n",
       "7        2.366219         0.780345  \n",
       "6        2.403737         0.780260  \n",
       "4        2.616092         0.780236  \n",
       "0        3.653663         0.771314  \n",
       "21       0.064902         0.756486  \n",
       "22       0.063360         0.725323  \n",
       "16       0.063463         0.704219  \n",
       "19       0.065689         0.702678  \n",
       "18       0.062805         0.658448  \n",
       "20       0.064460         0.613555  \n",
       "17       0.063135         0.598527  \n",
       "23       0.063774         0.493821  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print('На стеммах:')\n",
    "cv_sresult, sbest_estim = search_model(pipe, pipe_grid, tf_train, y_train, tf_test, y_test)\n",
    "cv_sresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872478854912166"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Блендинг?\n",
    "\n",
    "model_2 = SGDClassifier(class_weight='balanced', max_iter=5000, \n",
    "                        learning_rate='adaptive', early_stopping=True, \n",
    "                        eta0=2, random_state=42, loss='squared_hinge')\n",
    "model_2.fit(tf_train, y_train)\n",
    "\n",
    "f1_score(y_test, \n",
    "        ((sbest_estim.predict(tf_test) \n",
    "          + \n",
    "          model_2.predict(tf_test)\n",
    "         )*.5).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Теперь можно попробовать BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79785, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers as ppb\n",
    "from tqdm import notebook\n",
    "\n",
    "#готовенький берт из коробочки\n",
    "tokenizer = ppb.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model_bert = ppb.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#Попробую хотя бы на половине данных.\n",
    "df_bert1, df_bert2 = train_test_split(data, test_size=0.5, random_state=42)\n",
    "df_bert1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 1.62 s, total: 3min 10s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# токенизируем текст, максимальную длинну вектора фиксируем на 300, т.к. эта модель не вынесет больше 512.\n",
    "max_len = 300       \n",
    "tokenized = df_bert1['text'].apply(lambda x: tokenizer.encode(x, max_length=max_len, add_special_tokens=True))\n",
    "# max_len = max([len(i) for i in tokenized])\n",
    "\n",
    "# применим padding к векторам\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d22976bf608455fb216d974c574f1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100                                                            # размер батча.\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):               # Это количество эпох\n",
    "    \n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])     # Батч, буквально срез вектора.\n",
    "        attention_mask_batch = torch.LongTensor(                            # LongTensor это тип данных, а можно не лонг?\n",
    "            attention_mask[batch_size*i:batch_size*(i+1)])                  # маска вектора, теперь маска батча\n",
    "        \n",
    "        with torch.no_grad():                                               # ??? типа отрубили автоградиент?\n",
    "            batch_embeddings = model_bert(                                  # смотрим слова эмбединги\n",
    "                batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())               # что в остальных?\n",
    "\n",
    "feat_bert = np.concatenate(embeddings)\n",
    "feat_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ориентировочное время завершения 50 часов и ядро постоянно падает... терпиливым быть конечно хорошо, но всему есть предел)\n",
    "\n",
    "И у меня совершенно нет идей как ускорить процесс эмбеддинга..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_bert, df_bert1['toxic'], test_size=0.33, random_state=42)\n",
    "\n",
    "model_clf = LogisticRegression(random_state=42, solver='sag')\n",
    "print('F1 Cross_val_score: ', cross_val_score(model_clf, X_train, y_train, cv=5, scoring='f1').mean())\n",
    "print('F1 Test score: ', f1_score(model_clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Все очень сильно зависит от предобработки и полученния признаков из tf-idf\n",
    "\n",
    "Хотя разница в точности моделей на леммах и стеммах несущественная, зато лучшее время показывает лемматизация без тэгов и фиксированным парт'оф'спич, почти в 2 раза быстрее стемминга.\n",
    "\n",
    "При увеличении кол-ва признаков некоторые модели теряли в качестве почти в 2 раза. Заоблачные количества признаков на N граммах тоже удивили. Биграмм под 2кк, а триграмм аж 3кк... Мне казалось их должно становиться меньше, или он все со всеми склеивает? Может это потому что он стоп-слова не выкидывает?\n",
    "- Пробовал downsampling. \n",
    "\n",
    "Использовать с tf-idf нет смысла, upsempling тем более. Хотя, если применять его ко всему корпусу, то результат положительный, но это неправильно, просто выкинуть 30% данных с 0 классом... \n",
    "\n",
    "Без downsampling'а самая лучшая модель с балансировкой классов давала на трейне ~0.75, на тесте 0.7469. С ним, примененным только к трейну 0.792, а на тесте 0.739, уже похоже на переобучение. С ним, примененным ко всему датасету, на трейне примерно также 0.79, зато на тесте уже около 0.77.\n",
    "\n",
    "- Наивныей Байес Бернулли замечательно обучается, но на тесте у него ничего не выходит. Похоже на переобучение. Поэтому взял MultinomialNB, чтобы в топ не выскакивал.\n",
    "- Дерево и Лес не стал использовать т.к. это занимает очень много времени, результат скорее всего будет немногим лучше, а в случае с деревом возможно и хуже.\n",
    "- Лучшего токенайзера для английского чем split() я не нашел. Не мог решить проблему \"don't\" -> 'do', \"n't\". На самом деле без разницы, они все равно должны были попасть в стоп-слова, хотя на N-граммах это могло отразиться 2 лишними итерациями.\n",
    "- Bert. Bert штука мощная. С такой в тренажере особо не развернешься."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
